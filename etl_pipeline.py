import osimport jsonfrom pyspark.sql import SparkSessionfrom alert_rules import check_alerts# Initialize Sparkspark = SparkSession.builder \    .appName("Healthcare Alerts ETL") \    .master("local[*]") \    .getOrCreate()def load_sample_data(folder_path):    data = []    for filename in os.listdir(folder_path):        if filename.endswith(".json"):            with open(os.path.join(folder_path, filename)) as f:                record = json.load(f)                data.append(record)    return datadef main():    print("ðŸš€ Starting ETL Pipeline...\n")    raw_data = load_sample_data("sample_data")    df = spark.createDataFrame(raw_data)    print("ðŸ“Š Raw Data:")    df.show(truncate=False)    print("ðŸ”Ž Running Alert Rules...")    for row in df.collect():        alerts = check_alerts(row.asDict())        for alert in alerts:            print(alert)    print("\nâœ… ETL Pipeline Completed.")if __name__ == "__main__":    main()